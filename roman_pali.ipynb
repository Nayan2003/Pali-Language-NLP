{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950fb0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# ğŸ”  Unicode Normalization\n",
    "# ------------------------------\n",
    "def normalize_unicode(text):\n",
    "    # Normalize to composed form (preserves diacritics like á¹ƒ, Ä, Ã±, á¹­, etc.)\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "# ------------------------------\n",
    "# 1ï¸âƒ£ WORD-BASED TOKENIZER (Preserving diacritics)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    # Normalize first\n",
    "    text = normalize_unicode(text)\n",
    "    # Match full Pali diacritic range + Latin letters + hyphen/apostrophe\n",
    "    pattern = r\"[A-Za-zÄ€ÄÄªÄ«ÅªÅ«á¹€á¹á¹„á¹…Ã‘Ã±á¹¬á¹­á¸Œá¸á¹†á¹‡á¸¶á¸·á¸ºá¸»ÅšÅ›á¹¢á¹£á¸¤á¸¥â€™']+\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    tokens = [t.lower() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2ï¸âƒ£ SENTENCE-BASED TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    text = normalize_unicode(text)\n",
    "    sentences = re.split(r'[.!?â€“]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3ï¸âƒ£ DOCUMENT-BASED TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4ï¸âƒ£ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 5ï¸âƒ£ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6ï¸âƒ£ WORDâ€“DOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0febc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Word Tokens (Doc1): ['eva', 'me', 'suta', 'eka', 'samaya', 'bhagavÄ', 'rÄjagahe', 'viharati', 'jÄ«vakassa', 'komÄrabhaccassa', 'ambavane'] ...\n",
      "\n",
      "ğŸ”¹ Sentence Tokens (Doc1): ['Evaá¹ƒ me sutaá¹ƒ â€“ ekaá¹ƒ samayaá¹ƒ bhagavÄ rÄjagahe viharati jÄ«vakassa komÄrabhaccassa ambavane']\n",
      "\n",
      "ğŸ”¹ Sorted Frequency (Doc1): [('eva', 1), ('me', 1), ('suta', 1), ('eka', 1), ('samaya', 1), ('bhagavÄ', 1), ('rÄjagahe', 1), ('viharati', 1), ('jÄ«vakassa', 1), ('komÄrabhaccassa', 1)]\n",
      "\n",
      "ğŸ”¹ Wordâ€“Document Matrix:\n",
      "                  Doc1  Doc2\n",
      "ajÄtasattu          0     1\n",
      "ambavane            1     0\n",
      "bhagavÄ             1     0\n",
      "cÄtumÄsiniyÄ        0     1\n",
      "eka                 1     0\n",
      "eva                 1     0\n",
      "jÄ«vakassa           1     0\n",
      "komudiyÄ            0     1\n",
      "komÄrabhaccassa     1     0\n",
      "me                  1     0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# ğŸ“„ Sample Roman Pali Docs\n",
    "# ------------------------------\n",
    "doc1 = \"\"\"Evaá¹ƒ me sutaá¹ƒ â€“ ekaá¹ƒ samayaá¹ƒ bhagavÄ rÄjagahe viharati jÄ«vakassa komÄrabhaccassa ambavane.\"\"\"\n",
    "doc2 = \"\"\"RÄjÄ mÄgadho ajÄtasattu vedehiputto tadahuposathe pannarase komudiyÄ cÄtumÄsiniyÄ puá¹‡á¹‡Äya puá¹‡á¹‡amÄya rattiyÄ.\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "docs = tokenize_documents([doc1, doc2])\n",
    "docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "sentences = tokenize_sentences(doc1)\n",
    "freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "print(\"ğŸ”¹ Word Tokens (Doc1):\", docs_tokens[0][:20], \"...\")\n",
    "print(\"\\nğŸ”¹ Sentence Tokens (Doc1):\", sentences[:2])\n",
    "print(\"\\nğŸ”¹ Sorted Frequency (Doc1):\", sorted_freqs[0][:10])\n",
    "print(\"\\nğŸ”¹ Wordâ€“Document Matrix:\\n\", matrix.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f033c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Word Tokens (Doc1): ['evaá¹ƒ', 'me', 'sutaá¹ƒ', 'ekaá¹ƒ', 'samayaá¹ƒ', 'bhagavÄ', 'rÄjagahe', 'viharati', 'jÄ«vakassa', 'komÄrabhaccassa', 'ambavane']\n",
      "\n",
      "ğŸ”¹ Sentence Tokens (Doc1): ['Evaá¹ƒ me sutaá¹ƒ', 'ekaá¹ƒ samayaá¹ƒ bhagavÄ rÄjagahe viharati jÄ«vakassa komÄrabhaccassa ambavane']\n",
      "\n",
      "ğŸ”¹ Sorted Frequency (Doc1): [('evaá¹ƒ', 1), ('me', 1), ('sutaá¹ƒ', 1), ('ekaá¹ƒ', 1), ('samayaá¹ƒ', 1), ('bhagavÄ', 1), ('rÄjagahe', 1), ('viharati', 1), ('jÄ«vakassa', 1), ('komÄrabhaccassa', 1), ('ambavane', 1)]\n",
      "\n",
      "ğŸ”¹ Wordâ€“Document Matrix:\n",
      "                  Doc1  Doc2\n",
      "ajÄtasattu          0     1\n",
      "ambavane            1     0\n",
      "bhagavÄ             1     0\n",
      "cÄtumÄsiniyÄ        0     1\n",
      "ekaá¹ƒ                1     0\n",
      "evaá¹ƒ                1     0\n",
      "jÄ«vakassa           1     0\n",
      "komudiyÄ            0     1\n",
      "komÄrabhaccassa     1     0\n",
      "me                  1     0\n",
      "mÄgadho             0     1\n",
      "pannarase           0     1\n",
      "puá¹‡á¹‡amÄya           0     1\n",
      "puá¹‡á¹‡Äya             0     1\n",
      "rattiyÄ             0     1\n",
      "rÄjagahe            1     0\n",
      "rÄjÄ                0     1\n",
      "samayaá¹ƒ             1     0\n",
      "sutaá¹ƒ               1     0\n",
      "tadahuposathe       0     1\n",
      "vedehiputto         0     1\n",
      "viharati            1     0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------------------\n",
    "# ğŸ”  Unicode Normalization Fix\n",
    "# ------------------------------\n",
    "def normalize_unicode(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    # Replace combining dot with proper 'á¹ƒ' character\n",
    "    text = re.sub(r\"m[\\u0307\\u0323\\u0310]\", \"á¹ƒ\", text)   # handle different dot forms\n",
    "    text = re.sub(r\"n[\\u0307\\u0323\\u0310]\", \"á¹‡\", text)   # just in case for similar issue\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# 1ï¸âƒ£ WORD TOKENIZER (Roman Pali-safe)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    text = normalize_unicode(text)\n",
    "    pattern = r\"[A-Za-zÄ€ÄÄªÄ«ÅªÅ«á¹€á¹á¹„á¹…Ã‘Ã±á¹¬á¹­á¸Œá¸á¹†á¹‡á¸¶á¸·á¸ºá¸»ÅšÅ›á¹¢á¹£á¸¤á¸¥á¹ƒâ€™']+\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    tokens = [t.lower() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2ï¸âƒ£ SENTENCE TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    text = normalize_unicode(text)\n",
    "    sentences = re.split(r'[.!?â€“]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3ï¸âƒ£ DOCUMENT TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4ï¸âƒ£ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 5ï¸âƒ£ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6ï¸âƒ£ WORDâ€“DOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ğŸ“„ Sample Roman Pali Docs\n",
    "# ------------------------------\n",
    "doc1 = \"\"\"Evaá¹ƒ me sutaá¹ƒ â€“ ekaá¹ƒ samayaá¹ƒ bhagavÄ rÄjagahe viharati jÄ«vakassa komÄrabhaccassa ambavane.\"\"\"\n",
    "doc2 = \"\"\"RÄjÄ mÄgadho ajÄtasattu vedehiputto tadahuposathe pannarase komudiyÄ cÄtumÄsiniyÄ puá¹‡á¹‡Äya puá¹‡á¹‡amÄya rattiyÄ.\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "docs = tokenize_documents([doc1, doc2])\n",
    "docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "sentences = tokenize_sentences(doc1)\n",
    "freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "print(\"ğŸ”¹ Word Tokens (Doc1):\", docs_tokens[0])\n",
    "print(\"\\nğŸ”¹ Sentence Tokens (Doc1):\", sentences)\n",
    "print(\"\\nğŸ”¹ Sorted Frequency (Doc1):\", sorted_freqs[0])\n",
    "print(\"\\nğŸ”¹ Wordâ€“Document Matrix:\\n\", matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd6664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
