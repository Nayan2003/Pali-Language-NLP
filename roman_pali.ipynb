{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950fb0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# 🔠 Unicode Normalization\n",
    "# ------------------------------\n",
    "def normalize_unicode(text):\n",
    "    # Normalize to composed form (preserves diacritics like ṃ, ā, ñ, ṭ, etc.)\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ WORD-BASED TOKENIZER (Preserving diacritics)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    # Normalize first\n",
    "    text = normalize_unicode(text)\n",
    "    # Match full Pali diacritic range + Latin letters + hyphen/apostrophe\n",
    "    pattern = r\"[A-Za-zĀāĪīŪūṀṁṄṅÑñṬṭḌḍṆṇḶḷḺḻŚśṢṣḤḥ’']+\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    tokens = [t.lower() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ SENTENCE-BASED TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    text = normalize_unicode(text)\n",
    "    sentences = re.split(r'[.!?–]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ DOCUMENT-BASED TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ WORD–DOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0febc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Word Tokens (Doc1): ['eva', 'me', 'suta', 'eka', 'samaya', 'bhagavā', 'rājagahe', 'viharati', 'jīvakassa', 'komārabhaccassa', 'ambavane'] ...\n",
      "\n",
      "🔹 Sentence Tokens (Doc1): ['Evaṃ me sutaṃ – ekaṃ samayaṃ bhagavā rājagahe viharati jīvakassa komārabhaccassa ambavane']\n",
      "\n",
      "🔹 Sorted Frequency (Doc1): [('eva', 1), ('me', 1), ('suta', 1), ('eka', 1), ('samaya', 1), ('bhagavā', 1), ('rājagahe', 1), ('viharati', 1), ('jīvakassa', 1), ('komārabhaccassa', 1)]\n",
      "\n",
      "🔹 Word–Document Matrix:\n",
      "                  Doc1  Doc2\n",
      "ajātasattu          0     1\n",
      "ambavane            1     0\n",
      "bhagavā             1     0\n",
      "cātumāsiniyā        0     1\n",
      "eka                 1     0\n",
      "eva                 1     0\n",
      "jīvakassa           1     0\n",
      "komudiyā            0     1\n",
      "komārabhaccassa     1     0\n",
      "me                  1     0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# 📄 Sample Roman Pali Docs\n",
    "# ------------------------------\n",
    "doc1 = \"\"\"Evaṃ me sutaṃ – ekaṃ samayaṃ bhagavā rājagahe viharati jīvakassa komārabhaccassa ambavane.\"\"\"\n",
    "doc2 = \"\"\"Rājā māgadho ajātasattu vedehiputto tadahuposathe pannarase komudiyā cātumāsiniyā puṇṇāya puṇṇamāya rattiyā.\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "docs = tokenize_documents([doc1, doc2])\n",
    "docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "sentences = tokenize_sentences(doc1)\n",
    "freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "print(\"🔹 Word Tokens (Doc1):\", docs_tokens[0][:20], \"...\")\n",
    "print(\"\\n🔹 Sentence Tokens (Doc1):\", sentences[:2])\n",
    "print(\"\\n🔹 Sorted Frequency (Doc1):\", sorted_freqs[0][:10])\n",
    "print(\"\\n🔹 Word–Document Matrix:\\n\", matrix.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f033c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Word Tokens (Doc1): ['evaṃ', 'me', 'sutaṃ', 'ekaṃ', 'samayaṃ', 'bhagavā', 'rājagahe', 'viharati', 'jīvakassa', 'komārabhaccassa', 'ambavane']\n",
      "\n",
      "🔹 Sentence Tokens (Doc1): ['Evaṃ me sutaṃ', 'ekaṃ samayaṃ bhagavā rājagahe viharati jīvakassa komārabhaccassa ambavane']\n",
      "\n",
      "🔹 Sorted Frequency (Doc1): [('evaṃ', 1), ('me', 1), ('sutaṃ', 1), ('ekaṃ', 1), ('samayaṃ', 1), ('bhagavā', 1), ('rājagahe', 1), ('viharati', 1), ('jīvakassa', 1), ('komārabhaccassa', 1), ('ambavane', 1)]\n",
      "\n",
      "🔹 Word–Document Matrix:\n",
      "                  Doc1  Doc2\n",
      "ajātasattu          0     1\n",
      "ambavane            1     0\n",
      "bhagavā             1     0\n",
      "cātumāsiniyā        0     1\n",
      "ekaṃ                1     0\n",
      "evaṃ                1     0\n",
      "jīvakassa           1     0\n",
      "komudiyā            0     1\n",
      "komārabhaccassa     1     0\n",
      "me                  1     0\n",
      "māgadho             0     1\n",
      "pannarase           0     1\n",
      "puṇṇamāya           0     1\n",
      "puṇṇāya             0     1\n",
      "rattiyā             0     1\n",
      "rājagahe            1     0\n",
      "rājā                0     1\n",
      "samayaṃ             1     0\n",
      "sutaṃ               1     0\n",
      "tadahuposathe       0     1\n",
      "vedehiputto         0     1\n",
      "viharati            1     0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------------------\n",
    "# 🔠 Unicode Normalization Fix\n",
    "# ------------------------------\n",
    "def normalize_unicode(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    # Replace combining dot with proper 'ṃ' character\n",
    "    text = re.sub(r\"m[\\u0307\\u0323\\u0310]\", \"ṃ\", text)   # handle different dot forms\n",
    "    text = re.sub(r\"n[\\u0307\\u0323\\u0310]\", \"ṇ\", text)   # just in case for similar issue\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ WORD TOKENIZER (Roman Pali-safe)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    text = normalize_unicode(text)\n",
    "    pattern = r\"[A-Za-zĀāĪīŪūṀṁṄṅÑñṬṭḌḍṆṇḶḷḺḻŚśṢṣḤḥṃ’']+\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    tokens = [t.lower() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ SENTENCE TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    text = normalize_unicode(text)\n",
    "    sentences = re.split(r'[.!?–]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ DOCUMENT TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6️⃣ WORD–DOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 📄 Sample Roman Pali Docs\n",
    "# ------------------------------\n",
    "doc1 = \"\"\"Evaṃ me sutaṃ – ekaṃ samayaṃ bhagavā rājagahe viharati jīvakassa komārabhaccassa ambavane.\"\"\"\n",
    "doc2 = \"\"\"Rājā māgadho ajātasattu vedehiputto tadahuposathe pannarase komudiyā cātumāsiniyā puṇṇāya puṇṇamāya rattiyā.\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "docs = tokenize_documents([doc1, doc2])\n",
    "docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "sentences = tokenize_sentences(doc1)\n",
    "freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "print(\"🔹 Word Tokens (Doc1):\", docs_tokens[0])\n",
    "print(\"\\n🔹 Sentence Tokens (Doc1):\", sentences)\n",
    "print(\"\\n🔹 Sorted Frequency (Doc1):\", sorted_freqs[0])\n",
    "print(\"\\n🔹 Word–Document Matrix:\\n\", matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd6664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
