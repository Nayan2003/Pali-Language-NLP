{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11bae597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27489bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1Ô∏è‚É£ WORD-BASED TOKENIZER (Improved for Devanagari/Pali)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    # Match sequences of Devanagari letters + diacritics + numbers\n",
    "    # tokens = re.findall(r'[‡§Ö-‡§π‡§º-‡•â‡§º‡•ç‡§Å]+', text)\n",
    "    tokens = re.findall(r'[‡§Ö-‡§π‡§Ä-‡•£‡§º‡§º‡§Ω-‡§Ω‡§Å‡§Ç‡§É]+', text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2Ô∏è‚É£ SENTENCE-BASED TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    # Split by danda '‡•§', double danda '‡••', or standard punctuation\n",
    "    sentences = re.split(r'[‡•§‡••.!?‚Ä¶]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3Ô∏è‚É£ DOCUMENT-BASED TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4Ô∏è‚É£ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    freq_dict = Counter(tokens)\n",
    "    return freq_dict\n",
    "\n",
    "# ------------------------------\n",
    "# 5Ô∏è‚É£ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6Ô∏è‚É£ WORD‚ÄìDOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeed8fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Word Tokens (Doc1): ['‡§è‡§µ‡§Ç', '‡§Æ‡•á', '‡§∏‡•Å‡§§‡§Ç', '‡§è‡§ï‡§Ç', '‡§∏‡§Æ‡§Ø‡§Ç', '‡§≠‡§ó‡§µ‡§æ', '‡§∞‡§æ‡§ú‡§ó‡§π‡•á', '‡§µ‡§ø‡§π‡§∞‡§§‡§ø', '‡§§‡•á‡§®', '‡§∏‡§Æ‡§Ø‡•á‡§®']\n",
      "\n",
      "üîπ Sentence Tokens (Doc1): ['‡§è‡§µ‡§Ç ‡§Æ‡•á ‡§∏‡•Å‡§§‡§Ç ‚Äì ‡§è‡§ï‡§Ç ‡§∏‡§Æ‡§Ø‡§Ç ‡§≠‡§ó‡§µ‡§æ ‡§∞‡§æ‡§ú‡§ó‡§π‡•á ‡§µ‡§ø‡§π‡§∞‡§§‡§ø', '‡§§‡•á‡§® ‡§∏‡§Æ‡§Ø‡•á‡§®']\n",
      "\n",
      "üîπ Sorted Frequency (Doc1): [('‡§è‡§µ‡§Ç', 1), ('‡§Æ‡•á', 1), ('‡§∏‡•Å‡§§‡§Ç', 1), ('‡§è‡§ï‡§Ç', 1), ('‡§∏‡§Æ‡§Ø‡§Ç', 1), ('‡§≠‡§ó‡§µ‡§æ', 1), ('‡§∞‡§æ‡§ú‡§ó‡§π‡•á', 1), ('‡§µ‡§ø‡§π‡§∞‡§§‡§ø', 1), ('‡§§‡•á‡§®', 1), ('‡§∏‡§Æ‡§Ø‡•á‡§®', 1)]\n",
      "\n",
      "üîπ Word‚ÄìDocument Matrix:\n",
      "               Doc1  Doc2\n",
      "‡§Ö‡§°‡•ç‡§¢‡§§‡•á‡§≥‡§∏‡•á‡§π‡§ø      0     1\n",
      "‡§è‡§ï‡§Ç              1     0\n",
      "‡§è‡§µ‡§Ç              1     1\n",
      "‡§§‡•á‡§®              1     0\n",
      "‡§≠‡§ó‡§µ‡§æ             1     0\n",
      "‡§≠‡§ø‡§ï‡•ç‡§ñ‡•Å‡§∏‡§ô‡•ç‡§ò‡•á‡§®     0     1\n",
      "‡§≠‡§ø‡§ï‡•ç‡§ñ‡•Å‡§∏‡§§‡•á‡§π‡§ø      0     1\n",
      "‡§Æ‡•á               1     1\n",
      "‡§∞‡§æ‡§ú‡§ó‡§π‡•á           1     0\n",
      "‡§µ‡§ø‡§π‡§∞‡§§‡§ø           1     0\n",
      "‡§∏‡§¶‡•ç‡§ß‡§ø‡§Ç           0     1\n",
      "‡§∏‡§Æ‡§Ø‡§Ç             1     0\n",
      "‡§∏‡§Æ‡§Ø‡•á‡§®            1     0\n",
      "‡§∏‡•Å‡§§‡§Ç             1     1\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Sample raw Pali texts (unclean)\n",
    "# ------------------------------\n",
    "doc1 = \"‡§è‡§µ‡§Ç ‡§Æ‡•á ‡§∏‡•Å‡§§‡§Ç ‚Äì ‡§è‡§ï‡§Ç ‡§∏‡§Æ‡§Ø‡§Ç ‡§≠‡§ó‡§µ‡§æ ‡§∞‡§æ‡§ú‡§ó‡§π‡•á ‡§µ‡§ø‡§π‡§∞‡§§‡§ø‡•§ ‡§§‡•á‡§® ‡§∏‡§Æ‡§Ø‡•á‡§®...\"\n",
    "doc2 = \"‡§≠‡§ø‡§ï‡•ç‡§ñ‡•Å‡§∏‡§ô‡•ç‡§ò‡•á‡§® ‡§∏‡§¶‡•ç‡§ß‡§ø‡§Ç ‡§Ö‡§°‡•ç‡§¢‡§§‡•á‡§≥‡§∏‡•á‡§π‡§ø ‡§≠‡§ø‡§ï‡•ç‡§ñ‡•Å‡§∏‡§§‡•á‡§π‡§ø‡•• ‡§è‡§µ‡§Ç ‡§Æ‡•á ‡§∏‡•Å‡§§‡§Ç‡•§\"\n",
    "\n",
    "# Treat each doc as one document\n",
    "docs = tokenize_documents([doc1, doc2])\n",
    "\n",
    "# Tokenize words in each document\n",
    "docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "\n",
    "# Sentence-level tokens\n",
    "sentences = tokenize_sentences(doc1)\n",
    "\n",
    "# Frequency + Matrix\n",
    "freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "print(\"üîπ Word Tokens (Doc1):\", docs_tokens[0])\n",
    "print(\"\\nüîπ Sentence Tokens (Doc1):\", sentences)\n",
    "print(\"\\nüîπ Sorted Frequency (Doc1):\", sorted_freqs[0])\n",
    "print(\"\\nüîπ Word‚ÄìDocument Matrix:\\n\", matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62db9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
