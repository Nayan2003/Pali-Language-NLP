{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d48db38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ------------------------------\\n# ‚öôÔ∏è MAIN EXECUTION (Updated)\\n# ------------------------------\\nif __name__ == \"__main__\":\\n    print(\"üî∏ Pali Roman Script Text Processor üî∏\\n\")\\n\\n    docs = load_documents_interactively()\\n\\n    if not docs:\\n        print(\"‚ùå No valid documents loaded. Exiting...\")\\n    else:\\n        docs_tokens = [tokenize_words(doc) for doc in docs]\\n        docs_sentences = [tokenize_sentences(doc) for doc in docs]\\n        freqs = [word_frequency(tokens) for tokens in docs_tokens]\\n        sorted_freqs = [sort_by_frequency(f) for f in freqs]\\n        matrix = build_word_document_matrix(docs_tokens)\\n\\n        # Process each document\\n        for i, (tokens, sentences) in enumerate(zip(docs_tokens, docs_sentences)):\\n            total_tokens = len(tokens)\\n            unique_tokens = len(set(tokens))\\n            top10 = sorted_freqs[i][:10]\\n\\n            print(f\"\\nüìò Document {i+1} Summary:\")\\n            print(f\"   üîπ Total Tokens: {total_tokens}\")\\n            print(f\"   üîπ Unique Tokens: {unique_tokens}\")\\n\\n            print(f\"   üîπ Top 10 Frequent Words: {top10}\")\\n            print(f\"   üîπ Example Sentences: {sentences[:3]}\\n\")\\n\\n            # Export outputs\\n            export_results(i + 1, tokens, sentences, sorted_freqs[i])\\n\\n        # Show preview of matrix\\n        print(\"\\nüîπ Word‚ÄìDocument Matrix (Top 10):\\n\", matrix.head(10))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# üî† Unicode Normalization Fix\n",
    "# ------------------------------\n",
    "def normalize_unicode(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r\"m[\\u0307\\u0323\\u0310]\", \"·πÉ\", text)\n",
    "    text = re.sub(r\"n[\\u0307\\u0323\\u0310]\", \"·πá\", text)\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# 1Ô∏è‚É£ WORD TOKENIZER (Roman Pali-safe)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    text = normalize_unicode(text)\n",
    "    pattern = r\"[A-Za-zƒÄƒÅƒ™ƒ´≈™≈´·πÄ·πÅ·πÑ·πÖ√ë√±·π¨·π≠·∏å·∏ç·πÜ·πá·∏∂·∏∑·∏∫·∏ª≈ö≈õ·π¢·π£·∏§·∏•·πÉ‚Äô']+\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    tokens = [t.lower() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2Ô∏è‚É£ SENTENCE TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    text = normalize_unicode(text)\n",
    "    sentences = re.split(r'[.!?‚Äì]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3Ô∏è‚É£ DOCUMENT TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4Ô∏è‚É£ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 5Ô∏è‚É£ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6Ô∏è‚É£ WORD‚ÄìDOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n",
    "\n",
    "# ------------------------------\n",
    "# 7Ô∏è‚É£ LOAD DOCUMENTS (Ask User)\n",
    "# ------------------------------\n",
    "def load_documents_interactively():\n",
    "    docs = []\n",
    "    while True:\n",
    "        num_files_input = input(\"üìÇ How many text documents do you want to load? ‚û§ \").strip()\n",
    "        if not num_files_input.isdigit():\n",
    "            print(\"‚ö†Ô∏è Please enter a valid number (e.g., 1, 2, 3).\")\n",
    "            continue\n",
    "        num_files = int(num_files_input)\n",
    "        break\n",
    "\n",
    "    for i in range(num_files):\n",
    "        file_path = input(f\"üëâ Enter path for document {i+1} (e.g., doc{i+1}.txt): \").strip()\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                docs.append(content)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_path}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# üßæ EXPORT RESULTS\n",
    "# ------------------------------\n",
    "def export_results(doc_number, tokens, sentences, sorted_freq):\n",
    "    df_freq = pd.DataFrame(sorted_freq, columns=['Word', 'Frequency'])\n",
    "    df_tokens = pd.DataFrame(tokens, columns=['Token'])\n",
    "    df_sentences = pd.DataFrame(sentences, columns=['Sentence'])\n",
    "\n",
    "    # Export each as CSV\n",
    "    df_freq.to_csv(f'doc{doc_number}_freq.csv', index=False, encoding='utf-8')\n",
    "    df_tokens.to_csv(f'doc{doc_number}_tokens.csv', index=False, encoding='utf-8')\n",
    "    df_sentences.to_csv(f'doc{doc_number}_sentences.csv', index=False, encoding='utf-8')\n",
    "    print(f\"üìÇ Exported results for Document {doc_number} ‚úÖ\")\n",
    "\n",
    "'''\n",
    "# ------------------------------\n",
    "# ‚öôÔ∏è MAIN EXECUTION (Updated)\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üî∏ Pali Roman Script Text Processor üî∏\\n\")\n",
    "\n",
    "    docs = load_documents_interactively()\n",
    "\n",
    "    if not docs:\n",
    "        print(\"‚ùå No valid documents loaded. Exiting...\")\n",
    "    else:\n",
    "        docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "        docs_sentences = [tokenize_sentences(doc) for doc in docs]\n",
    "        freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "        sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "        matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "        # Process each document\n",
    "        for i, (tokens, sentences) in enumerate(zip(docs_tokens, docs_sentences)):\n",
    "            total_tokens = len(tokens)\n",
    "            unique_tokens = len(set(tokens))\n",
    "            top10 = sorted_freqs[i][:10]\n",
    "\n",
    "            print(f\"\\nüìò Document {i+1} Summary:\")\n",
    "            print(f\"   üîπ Total Tokens: {total_tokens}\")\n",
    "            print(f\"   üîπ Unique Tokens: {unique_tokens}\")\n",
    "\n",
    "            print(f\"   üîπ Top 10 Frequent Words: {top10}\")\n",
    "            print(f\"   üîπ Example Sentences: {sentences[:3]}\\n\")\n",
    "\n",
    "            # Export outputs\n",
    "            export_results(i + 1, tokens, sentences, sorted_freqs[i])\n",
    "\n",
    "        # Show preview of matrix\n",
    "        print(\"\\nüîπ Word‚ÄìDocument Matrix (Top 10):\\n\", matrix.head(10))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521871e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ Pali Roman Script Text Processor üî∏\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üìÇ How many text documents do you want to load? ‚û§  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Please enter a valid number (e.g., 1, 2, 3).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üìÇ How many text documents do you want to load? ‚û§  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Please enter a valid number (e.g., 1, 2, 3).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üìÇ How many text documents do you want to load? ‚û§  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Please enter a valid number (e.g., 1, 2, 3).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üìÇ How many text documents do you want to load? ‚û§  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Please enter a valid number (e.g., 1, 2, 3).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üìÇ How many text documents do you want to load? ‚û§  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Please enter a valid number (e.g., 1, 2, 3).\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# ‚öôÔ∏è MAIN EXECUTION (Updated)\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üî∏ Pali Roman Script Text Processor üî∏\\n\")\n",
    "\n",
    "    docs = load_documents_interactively()\n",
    "\n",
    "    if not docs:\n",
    "        print(\"‚ùå No valid documents loaded. Exiting...\")\n",
    "    else:\n",
    "        docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "        docs_sentences = [tokenize_sentences(doc) for doc in docs]\n",
    "        freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "        sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "        matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "        # Process each document\n",
    "        for i, (tokens, sentences) in enumerate(zip(docs_tokens, docs_sentences)):\n",
    "                total_tokens = len(tokens)\n",
    "                unique_tokens = len(set(tokens))\n",
    "                total_sentences = len(sentences)   # ‚úÖ Count of sentences\n",
    "                top10 = sorted_freqs[i][:10]\n",
    "\n",
    "                print(f\"\\nüìò Document {i+1} Summary:\")\n",
    "                print(f\"   üîπ Total Tokens: {total_tokens}\")\n",
    "                print(f\"   üîπ Unique Tokens: {unique_tokens}\")      # Unique word tokens\n",
    "                print(f\"   üîπ Total Sentences: {total_sentences}\") # Sentence tokens count\n",
    "                print(f\"   üîπ Top 10 Frequent Words: {top10}\")\n",
    "                print(f\"   üîπ Example Sentences: {sentences[:3]}\\n\")\n",
    "\n",
    "                # Export outputs\n",
    "                export_results(i + 1, tokens, sentences, sorted_freqs[i])\n",
    "\n",
    "        # Show preview of matrix\n",
    "        print(\"\\nüîπ Word‚ÄìDocument Matrix (Top 10):\\n\", matrix.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e208b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
