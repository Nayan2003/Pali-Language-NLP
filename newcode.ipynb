{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d48db38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ------------------------------\\n# âš™ï¸ MAIN EXECUTION (Updated)\\n# ------------------------------\\nif __name__ == \"__main__\":\\n    print(\"ğŸ”¸ Pali Roman Script Text Processor ğŸ”¸\\n\")\\n\\n    docs = load_documents_interactively()\\n\\n    if not docs:\\n        print(\"âŒ No valid documents loaded. Exiting...\")\\n    else:\\n        docs_tokens = [tokenize_words(doc) for doc in docs]\\n        docs_sentences = [tokenize_sentences(doc) for doc in docs]\\n        freqs = [word_frequency(tokens) for tokens in docs_tokens]\\n        sorted_freqs = [sort_by_frequency(f) for f in freqs]\\n        matrix = build_word_document_matrix(docs_tokens)\\n\\n        # Process each document\\n        for i, (tokens, sentences) in enumerate(zip(docs_tokens, docs_sentences)):\\n            total_tokens = len(tokens)\\n            unique_tokens = len(set(tokens))\\n            top10 = sorted_freqs[i][:10]\\n\\n            print(f\"\\nğŸ“˜ Document {i+1} Summary:\")\\n            print(f\"   ğŸ”¹ Total Tokens: {total_tokens}\")\\n            print(f\"   ğŸ”¹ Unique Tokens: {unique_tokens}\")\\n\\n            print(f\"   ğŸ”¹ Top 10 Frequent Words: {top10}\")\\n            print(f\"   ğŸ”¹ Example Sentences: {sentences[:3]}\\n\")\\n\\n            # Export outputs\\n            export_results(i + 1, tokens, sentences, sorted_freqs[i])\\n\\n        # Show preview of matrix\\n        print(\"\\nğŸ”¹ Wordâ€“Document Matrix (Top 10):\\n\", matrix.head(10))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ğŸ”  Unicode Normalization Fix\n",
    "# ------------------------------\n",
    "def normalize_unicode(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r\"m[\\u0307\\u0323\\u0310]\", \"á¹ƒ\", text)\n",
    "    text = re.sub(r\"n[\\u0307\\u0323\\u0310]\", \"á¹‡\", text)\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# 1ï¸âƒ£ WORD TOKENIZER (Roman Pali-safe)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    text = normalize_unicode(text)\n",
    "    pattern = r\"[A-Za-zÄ€ÄÄªÄ«ÅªÅ«á¹€á¹á¹„á¹…Ã‘Ã±á¹¬á¹­á¸Œá¸á¹†á¹‡á¸¶á¸·á¸ºá¸»ÅšÅ›á¹¢á¹£á¸¤á¸¥á¹ƒâ€™']+\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    tokens = [t.lower() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2ï¸âƒ£ SENTENCE TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    text = normalize_unicode(text)\n",
    "    sentences = re.split(r'[.!?â€“]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3ï¸âƒ£ DOCUMENT TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4ï¸âƒ£ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 5ï¸âƒ£ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6ï¸âƒ£ WORDâ€“DOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n",
    "\n",
    "# ------------------------------\n",
    "# 7ï¸âƒ£ LOAD DOCUMENTS (Ask User)\n",
    "# ------------------------------\n",
    "def load_documents_interactively():\n",
    "    docs = []\n",
    "    while True:\n",
    "        num_files_input = input(\"ğŸ“‚ How many text documents do you want to load? â¤ \").strip()\n",
    "        if not num_files_input.isdigit():\n",
    "            print(\"âš ï¸ Please enter a valid number (e.g., 1, 2, 3).\")\n",
    "            continue\n",
    "        num_files = int(num_files_input)\n",
    "        break\n",
    "\n",
    "    for i in range(num_files):\n",
    "        file_path = input(f\"ğŸ‘‰ Enter path for document {i+1} (e.g., doc{i+1}.txt): \").strip()\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                docs.append(content)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ File not found: {file_path}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ğŸ§¾ EXPORT RESULTS\n",
    "# ------------------------------\n",
    "def export_results(doc_number, tokens, sentences, sorted_freq):\n",
    "    df_freq = pd.DataFrame(sorted_freq, columns=['Word', 'Frequency'])\n",
    "    df_tokens = pd.DataFrame(tokens, columns=['Token'])\n",
    "    df_sentences = pd.DataFrame(sentences, columns=['Sentence'])\n",
    "\n",
    "    # Export each as CSV\n",
    "    df_freq.to_csv(f'doc{doc_number}_freq.csv', index=False, encoding='utf-8')\n",
    "    df_tokens.to_csv(f'doc{doc_number}_tokens.csv', index=False, encoding='utf-8')\n",
    "    df_sentences.to_csv(f'doc{doc_number}_sentences.csv', index=False, encoding='utf-8')\n",
    "    print(f\"ğŸ“‚ Exported results for Document {doc_number} âœ…\")\n",
    "\n",
    "'''\n",
    "# ------------------------------\n",
    "# âš™ï¸ MAIN EXECUTION (Updated)\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ”¸ Pali Roman Script Text Processor ğŸ”¸\\n\")\n",
    "\n",
    "    docs = load_documents_interactively()\n",
    "\n",
    "    if not docs:\n",
    "        print(\"âŒ No valid documents loaded. Exiting...\")\n",
    "    else:\n",
    "        docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "        docs_sentences = [tokenize_sentences(doc) for doc in docs]\n",
    "        freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "        sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "        matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "        # Process each document\n",
    "        for i, (tokens, sentences) in enumerate(zip(docs_tokens, docs_sentences)):\n",
    "            total_tokens = len(tokens)\n",
    "            unique_tokens = len(set(tokens))\n",
    "            top10 = sorted_freqs[i][:10]\n",
    "\n",
    "            print(f\"\\nğŸ“˜ Document {i+1} Summary:\")\n",
    "            print(f\"   ğŸ”¹ Total Tokens: {total_tokens}\")\n",
    "            print(f\"   ğŸ”¹ Unique Tokens: {unique_tokens}\")\n",
    "\n",
    "            print(f\"   ğŸ”¹ Top 10 Frequent Words: {top10}\")\n",
    "            print(f\"   ğŸ”¹ Example Sentences: {sentences[:3]}\\n\")\n",
    "\n",
    "            # Export outputs\n",
    "            export_results(i + 1, tokens, sentences, sorted_freqs[i])\n",
    "\n",
    "        # Show preview of matrix\n",
    "        print(\"\\nğŸ”¹ Wordâ€“Document Matrix (Top 10):\\n\", matrix.head(10))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521871e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¸ Pali Roman Script Text Processor ğŸ”¸\n",
      "\n",
      "\n",
      "ğŸ“˜ Document 1 Summary:\n",
      "   ğŸ”¹ Total Tokens: 7174\n",
      "   ğŸ”¹ Unique Tokens: 2229\n",
      "   ğŸ”¹ Total Sentences: 819\n",
      "   ğŸ”¹ Top 10 Frequent Words: [('vÄ', 174), ('hoti', 127), ('kho', 120), ('mahÄrÄja', 108), ('so', 81), ('ca', 73), ('bhante', 73), ('evaá¹ƒ', 69), ('cittaá¹ƒ', 68), ('pajÄnÄti', 58)]\n",
      "   ğŸ”¹ Example Sentences: ['2', 'SÄmaÃ±Ã±aphalasuttaá¹ƒ\\n\\nRÄjÄmaccakathÄ\\n\\n150', 'Evaá¹ƒ me sutaá¹ƒ']\n",
      "\n",
      "ğŸ“‚ Exported results for Document 1 âœ…\n",
      "\n",
      "ğŸ”¹ Wordâ€“Document Matrix (Top 10):\n",
      "                Doc1\n",
      "abalÄ             1\n",
      "abbhantarÄnaá¹ƒ     4\n",
      "abbhayena         2\n",
      "abbhokÄsaá¹ƒ        1\n",
      "abbhokÄso         1\n",
      "abbhuggato        1\n",
      "abbhujjalanaá¹ƒ     1\n",
      "abbhutaá¹ƒ          2\n",
      "abhijjamÄne       2\n",
      "abhijjamÄno       1\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# âš™ï¸ MAIN EXECUTION (Updated)\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ”¸ Pali Roman Script Text Processor ğŸ”¸\\n\")\n",
    "\n",
    "    docs = load_documents_interactively()\n",
    "\n",
    "    if not docs:\n",
    "        print(\"âŒ No valid documents loaded. Exiting...\")\n",
    "    else:\n",
    "        docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "        docs_sentences = [tokenize_sentences(doc) for doc in docs]\n",
    "        freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "        sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "        matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "        # Process each document\n",
    "        for i, (tokens, sentences) in enumerate(zip(docs_tokens, docs_sentences)):\n",
    "                total_tokens = len(tokens)\n",
    "                unique_tokens = len(set(tokens))\n",
    "                total_sentences = len(sentences)   # âœ… Count of sentences\n",
    "                top10 = sorted_freqs[i][:10]\n",
    "\n",
    "                print(f\"\\nğŸ“˜ Document {i+1} Summary:\")\n",
    "                print(f\"   ğŸ”¹ Total Tokens: {total_tokens}\")\n",
    "                print(f\"   ğŸ”¹ Unique Tokens: {unique_tokens}\")      # Unique word tokens\n",
    "                print(f\"   ğŸ”¹ Total Sentences: {total_sentences}\") # Sentence tokens count\n",
    "                print(f\"   ğŸ”¹ Top 10 Frequent Words: {top10}\")\n",
    "                print(f\"   ğŸ”¹ Example Sentences: {sentences[:3]}\\n\")\n",
    "\n",
    "                # Export outputs\n",
    "                export_results(i + 1, tokens, sentences, sorted_freqs[i])\n",
    "\n",
    "        # Show preview of matrix\n",
    "        print(\"\\nğŸ”¹ Wordâ€“Document Matrix (Top 10):\\n\", matrix.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e208b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
