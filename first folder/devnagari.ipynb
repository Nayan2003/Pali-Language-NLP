{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb9fc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf06521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7791d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# ğŸ”  Unicode Normalization Fix\n",
    "# ------------------------------\n",
    "def normalize_unicode(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r\"m[\\u0307\\u0323\\u0310]\", \"á¹ƒ\", text)\n",
    "    text = re.sub(r\"n[\\u0307\\u0323\\u0310]\", \"á¹‡\", text)\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# 1ï¸âƒ£ WORD TOKENIZER (Roman Pali-safe)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    text = normalize_unicode(text)\n",
    "    pattern = r\"[A-Za-zÄ€ÄÄªÄ«ÅªÅ«á¹€á¹á¹„á¹…Ã‘Ã±á¹¬á¹­á¸Œá¸á¹†á¹‡á¸¶á¸·á¸ºá¸»ÅšÅ›á¹¢á¹£á¸¤á¸¥á¹ƒâ€™']+\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    tokens = [t.lower() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2ï¸âƒ£ SENTENCE TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    text = normalize_unicode(text)\n",
    "    sentences = re.split(r'[.!?â€“]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3ï¸âƒ£ DOCUMENT TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4ï¸âƒ£ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 5ï¸âƒ£ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6ï¸âƒ£ WORDâ€“DOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n",
    "\n",
    "# ------------------------------\n",
    "# 7ï¸âƒ£ LOAD DOCUMENTS (Ask User)\n",
    "# ------------------------------\n",
    "def load_documents_interactively():\n",
    "    docs = []\n",
    "    num_files = int(input(\"ğŸ“‚ How many text documents do you want to load? â¤ \"))\n",
    "\n",
    "    for i in range(num_files):\n",
    "        file_path = input(f\"ğŸ‘‰ Enter path for document {i+1} (e.g., doc{i+1}.txt): \").strip()\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                docs.append(content)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ File not found: {file_path}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c21b6",
   "metadata": {},
   "source": [
    "### error code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1556efa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¸ Pali Roman Script Text Processor ğŸ”¸\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ”¸ Pali Roman Script Text Processor ğŸ”¸\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load documents from user input\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m docs \u001b[38;5;241m=\u001b[39m load_documents_interactively()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m docs:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ No valid documents loaded. Exiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m, in \u001b[0;36mload_documents_interactively\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_documents_interactively\u001b[39m():\n\u001b[0;32m     64\u001b[0m     docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 65\u001b[0m     num_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“‚ How many text documents do you want to load? â¤ \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_files):\n\u001b[0;32m     68\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ‘‰ Enter path for document \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (e.g., doc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# âš™ï¸ MAIN EXECUTION\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ”¸ Pali Roman Script Text Processor ğŸ”¸\\n\")\n",
    "\n",
    "    # Load documents from user input\n",
    "    docs = load_documents_interactively()\n",
    "\n",
    "    if not docs:\n",
    "        print(\"âŒ No valid documents loaded. Exiting...\")\n",
    "    else:\n",
    "        # Tokenize and analyze\n",
    "        docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "        sentences = tokenize_sentences(docs[0])\n",
    "        freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "        sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "        matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\nâœ… Processing Completed!\\n\")\n",
    "        print(\"ğŸ”¹ Word Tokens (Doc1):\", docs_tokens[0][:20], \"...\")\n",
    "        print(\"\\nğŸ”¹ Sentence Tokens (Doc1):\", sentences[:3], \"...\")\n",
    "        print(\"\\nğŸ”¹ Sorted Frequency (Doc1):\", sorted_freqs[0][:10])\n",
    "        print(\"\\nğŸ”¹ Wordâ€“Document Matrix:\\n\", matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e49db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f81e4536",
   "metadata": {},
   "source": [
    "## code after error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ead2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf6b08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# ğŸ”  Unicode Normalization Fix\n",
    "# ------------------------------\n",
    "def normalize_unicode(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r\"m[\\u0307\\u0323\\u0310]\", \"á¹ƒ\", text)\n",
    "    text = re.sub(r\"n[\\u0307\\u0323\\u0310]\", \"á¹‡\", text)\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# 1ï¸âƒ£ WORD TOKENIZER (Roman Pali-safe)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    text = normalize_unicode(text)\n",
    "    pattern = r\"[A-Za-zÄ€ÄÄªÄ«ÅªÅ«á¹€á¹á¹„á¹…Ã‘Ã±á¹¬á¹­á¸Œá¸á¹†á¹‡á¸¶á¸·á¸ºá¸»ÅšÅ›á¹¢á¹£á¸¤á¸¥á¹ƒâ€™']+\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    tokens = [t.lower() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2ï¸âƒ£ SENTENCE TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    text = normalize_unicode(text)\n",
    "    sentences = re.split(r'[.!?â€“]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3ï¸âƒ£ DOCUMENT TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4ï¸âƒ£ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 5ï¸âƒ£ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6ï¸âƒ£ WORDâ€“DOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n",
    "\n",
    "# ------------------------------\n",
    "# 7ï¸âƒ£ LOAD DOCUMENTS (Ask User)\n",
    "# ------------------------------\n",
    "def load_documents_interactively():\n",
    "    docs = []\n",
    "    while True:\n",
    "        num_files_input = input(\"ğŸ“‚ How many text documents do you want to load? â¤ \").strip()\n",
    "        if not num_files_input.isdigit():\n",
    "            print(\"âš ï¸ Please enter a valid number (e.g., 1, 2, 3).\")\n",
    "            continue\n",
    "        num_files = int(num_files_input)\n",
    "        break\n",
    "\n",
    "    for i in range(num_files):\n",
    "        file_path = input(f\"ğŸ‘‰ Enter path for document {i+1} (e.g., doc{i+1}.txt): \").strip()\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                docs.append(content)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ File not found: {file_path}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce682f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¸ Pali Roman Script Text Processor ğŸ”¸\n",
      "\n",
      "\n",
      "âœ… Processing Completed!\n",
      "\n",
      "ğŸ”¹ Word Tokens (Doc1): ['sÄmaÃ±Ã±aphalasuttaá¹ƒ', 'rÄjÄmaccakathÄ', 'evaá¹ƒ', 'me', 'sutaá¹ƒ', 'ekaá¹ƒ', 'samayaá¹ƒ', 'bhagavÄ', 'rÄjagahe', 'viharati', 'jÄ«vakassa', 'komÄrabhaccassa', 'ambavane', 'mahatÄ', 'bhikkhusaá¹…ghena', 'saddhiá¹ƒ', 'aá¸á¸hateá¸·asehi', 'bhikkhusatehi', 'tena', 'kho'] ...\n",
      "\n",
      "ğŸ”¹ Sentence Tokens (Doc1): ['2', 'SÄmaÃ±Ã±aphalasuttaá¹ƒ\\n\\nRÄjÄmaccakathÄ\\n\\n150', 'Evaá¹ƒ me sutaá¹ƒ'] ...\n",
      "\n",
      "ğŸ”¹ Sorted Frequency (Doc1): [('vÄ', 174), ('hoti', 127), ('kho', 120), ('mahÄrÄja', 108), ('so', 81), ('ca', 73), ('bhante', 73), ('evaá¹ƒ', 69), ('cittaá¹ƒ', 68), ('pajÄnÄti', 58)]\n",
      "\n",
      "ğŸ”¹ Wordâ€“Document Matrix:\n",
      "                Doc1  Doc2\n",
      "abalÄ             1     0\n",
      "abbhantarÄnaá¹ƒ     4     0\n",
      "abbhayena         2     0\n",
      "abbhokÄsaá¹ƒ        1     0\n",
      "abbhokÄso         1     0\n",
      "...             ...   ...\n",
      "á¹­hitatto          1     0\n",
      "á¹­hite            17     0\n",
      "á¹­hito             3     0\n",
      "â€™                 0    94\n",
      "â€™â€™                0    48\n",
      "\n",
      "[2231 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# âš™ï¸ MAIN EXECUTION\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ”¸ Pali Roman Script Text Processor ğŸ”¸\\n\")\n",
    "\n",
    "    # Load documents from user input\n",
    "    docs = load_documents_interactively()\n",
    "\n",
    "    if not docs:\n",
    "        print(\"âŒ No valid documents loaded. Exiting...\")\n",
    "    else:\n",
    "        # Tokenize and analyze\n",
    "        docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "        sentences = tokenize_sentences(docs[0])\n",
    "        freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "        sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "        matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\nâœ… Processing Completed!\\n\")\n",
    "        print(\"ğŸ”¹ Word Tokens (Doc1):\", docs_tokens[0][:20], \"...\")\n",
    "        print(\"\\nğŸ”¹ Sentence Tokens (Doc1):\", sentences[:3], \"...\")\n",
    "        print(\"\\nğŸ”¹ Sorted Frequency (Doc1):\", sorted_freqs[0][:10])\n",
    "        print(\"\\nğŸ”¹ Wordâ€“Document Matrix:\\n\", matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b64f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
