{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb9fc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf06521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7791d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# üî† Unicode Normalization Fix\n",
    "# ------------------------------\n",
    "def normalize_unicode(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r\"m[\\u0307\\u0323\\u0310]\", \"·πÉ\", text)\n",
    "    text = re.sub(r\"n[\\u0307\\u0323\\u0310]\", \"·πá\", text)\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# 1Ô∏è‚É£ WORD TOKENIZER (Roman Pali-safe)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    text = normalize_unicode(text)\n",
    "    pattern = r\"[A-Za-zƒÄƒÅƒ™ƒ´≈™≈´·πÄ·πÅ·πÑ·πÖ√ë√±·π¨·π≠·∏å·∏ç·πÜ·πá·∏∂·∏∑·∏∫·∏ª≈ö≈õ·π¢·π£·∏§·∏•·πÉ‚Äô']+\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    tokens = [t.lower() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2Ô∏è‚É£ SENTENCE TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    text = normalize_unicode(text)\n",
    "    sentences = re.split(r'[.!?‚Äì]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3Ô∏è‚É£ DOCUMENT TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4Ô∏è‚É£ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 5Ô∏è‚É£ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6Ô∏è‚É£ WORD‚ÄìDOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n",
    "\n",
    "# ------------------------------\n",
    "# 7Ô∏è‚É£ LOAD DOCUMENTS (Ask User)\n",
    "# ------------------------------\n",
    "def load_documents_interactively():\n",
    "    docs = []\n",
    "    num_files = int(input(\"üìÇ How many text documents do you want to load? ‚û§ \"))\n",
    "\n",
    "    for i in range(num_files):\n",
    "        file_path = input(f\"üëâ Enter path for document {i+1} (e.g., doc{i+1}.txt): \").strip()\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                docs.append(content)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_path}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c21b6",
   "metadata": {},
   "source": [
    "### error code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1556efa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ Pali Roman Script Text Processor üî∏\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müî∏ Pali Roman Script Text Processor üî∏\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load documents from user input\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m docs \u001b[38;5;241m=\u001b[39m load_documents_interactively()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m docs:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå No valid documents loaded. Exiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m, in \u001b[0;36mload_documents_interactively\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_documents_interactively\u001b[39m():\n\u001b[0;32m     64\u001b[0m     docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 65\u001b[0m     num_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÇ How many text documents do you want to load? ‚û§ \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_files):\n\u001b[0;32m     68\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müëâ Enter path for document \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (e.g., doc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# ‚öôÔ∏è MAIN EXECUTION\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üî∏ Pali Roman Script Text Processor üî∏\\n\")\n",
    "\n",
    "    # Load documents from user input\n",
    "    docs = load_documents_interactively()\n",
    "\n",
    "    if not docs:\n",
    "        print(\"‚ùå No valid documents loaded. Exiting...\")\n",
    "    else:\n",
    "        # Tokenize and analyze\n",
    "        docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "        sentences = tokenize_sentences(docs[0])\n",
    "        freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "        sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "        matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n‚úÖ Processing Completed!\\n\")\n",
    "        print(\"üîπ Word Tokens (Doc1):\", docs_tokens[0][:20], \"...\")\n",
    "        print(\"\\nüîπ Sentence Tokens (Doc1):\", sentences[:3], \"...\")\n",
    "        print(\"\\nüîπ Sorted Frequency (Doc1):\", sorted_freqs[0][:10])\n",
    "        print(\"\\nüîπ Word‚ÄìDocument Matrix:\\n\", matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e49db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f81e4536",
   "metadata": {},
   "source": [
    "## code after error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ead2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf6b08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# üî† Unicode Normalization Fix\n",
    "# ------------------------------\n",
    "def normalize_unicode(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r\"m[\\u0307\\u0323\\u0310]\", \"·πÉ\", text)\n",
    "    text = re.sub(r\"n[\\u0307\\u0323\\u0310]\", \"·πá\", text)\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# 1Ô∏è‚É£ WORD TOKENIZER (Roman Pali-safe)\n",
    "# ------------------------------\n",
    "def tokenize_words(text):\n",
    "    text = normalize_unicode(text)\n",
    "    pattern = r\"[A-Za-zƒÄƒÅƒ™ƒ´≈™≈´·πÄ·πÅ·πÑ·πÖ√ë√±·π¨·π≠·∏å·∏ç·πÜ·πá·∏∂·∏∑·∏∫·∏ª≈ö≈õ·π¢·π£·∏§·∏•·πÉ‚Äô']+\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    tokens = [t.lower() for t in tokens if t.strip()]\n",
    "    return tokens\n",
    "\n",
    "# ------------------------------\n",
    "# 2Ô∏è‚É£ SENTENCE TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_sentences(text):\n",
    "    text = normalize_unicode(text)\n",
    "    sentences = re.split(r'[.!?‚Äì]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# ------------------------------\n",
    "# 3Ô∏è‚É£ DOCUMENT TOKENIZER\n",
    "# ------------------------------\n",
    "def tokenize_documents(texts):\n",
    "    return texts\n",
    "\n",
    "# ------------------------------\n",
    "# 4Ô∏è‚É£ WORD FREQUENCY COUNTER\n",
    "# ------------------------------\n",
    "def word_frequency(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# ------------------------------\n",
    "# 5Ô∏è‚É£ SORT BY FREQUENCY\n",
    "# ------------------------------\n",
    "def sort_by_frequency(freq_dict):\n",
    "    return sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 6Ô∏è‚É£ WORD‚ÄìDOCUMENT MATRIX\n",
    "# ------------------------------\n",
    "def build_word_document_matrix(docs_tokens):\n",
    "    all_words = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "    matrix_data = []\n",
    "    for word in all_words:\n",
    "        row = [tokens.count(word) for tokens in docs_tokens]\n",
    "        matrix_data.append(row)\n",
    "    df = pd.DataFrame(matrix_data, index=all_words,\n",
    "                      columns=[f'Doc{i+1}' for i in range(len(docs_tokens))])\n",
    "    return df\n",
    "\n",
    "# ------------------------------\n",
    "# 7Ô∏è‚É£ LOAD DOCUMENTS (Ask User)\n",
    "# ------------------------------\n",
    "def load_documents_interactively():\n",
    "    docs = []\n",
    "    while True:\n",
    "        num_files_input = input(\"üìÇ How many text documents do you want to load? ‚û§ \").strip()\n",
    "        if not num_files_input.isdigit():\n",
    "            print(\"‚ö†Ô∏è Please enter a valid number (e.g., 1, 2, 3).\")\n",
    "            continue\n",
    "        num_files = int(num_files_input)\n",
    "        break\n",
    "\n",
    "    for i in range(num_files):\n",
    "        file_path = input(f\"üëâ Enter path for document {i+1} (e.g., doc{i+1}.txt): \").strip()\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                docs.append(content)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_path}\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce682f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ Pali Roman Script Text Processor üî∏\n",
      "\n",
      "\n",
      "‚úÖ Processing Completed!\n",
      "\n",
      "üîπ Word Tokens (Doc1): ['sƒÅma√±√±aphalasutta·πÉ', 'rƒÅjƒÅmaccakathƒÅ', 'eva·πÉ', 'me', 'suta·πÉ', 'eka·πÉ', 'samaya·πÉ', 'bhagavƒÅ', 'rƒÅjagahe', 'viharati', 'jƒ´vakassa', 'komƒÅrabhaccassa', 'ambavane', 'mahatƒÅ', 'bhikkhusa·πÖghena', 'saddhi·πÉ', 'a·∏ç·∏çhate·∏∑asehi', 'bhikkhusatehi', 'tena', 'kho'] ...\n",
      "\n",
      "üîπ Sentence Tokens (Doc1): ['2', 'SƒÅma√±√±aphalasutta·πÉ\\n\\nRƒÅjƒÅmaccakathƒÅ\\n\\n150', 'Eva·πÉ me suta·πÉ'] ...\n",
      "\n",
      "üîπ Sorted Frequency (Doc1): [('vƒÅ', 174), ('hoti', 127), ('kho', 120), ('mahƒÅrƒÅja', 108), ('so', 81), ('ca', 73), ('bhante', 73), ('eva·πÉ', 69), ('citta·πÉ', 68), ('pajƒÅnƒÅti', 58)]\n",
      "\n",
      "üîπ Word‚ÄìDocument Matrix:\n",
      "                Doc1  Doc2\n",
      "abalƒÅ             1     0\n",
      "abbhantarƒÅna·πÉ     4     0\n",
      "abbhayena         2     0\n",
      "abbhokƒÅsa·πÉ        1     0\n",
      "abbhokƒÅso         1     0\n",
      "...             ...   ...\n",
      "·π≠hitatto          1     0\n",
      "·π≠hite            17     0\n",
      "·π≠hito             3     0\n",
      "‚Äô                 0    94\n",
      "‚Äô‚Äô                0    48\n",
      "\n",
      "[2231 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# ‚öôÔ∏è MAIN EXECUTION\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üî∏ Pali Roman Script Text Processor üî∏\\n\")\n",
    "\n",
    "    # Load documents from user input\n",
    "    docs = load_documents_interactively()\n",
    "\n",
    "    if not docs:\n",
    "        print(\"‚ùå No valid documents loaded. Exiting...\")\n",
    "    else:\n",
    "        # Tokenize and analyze\n",
    "        docs_tokens = [tokenize_words(doc) for doc in docs]\n",
    "        sentences = tokenize_sentences(docs[0])\n",
    "        freqs = [word_frequency(tokens) for tokens in docs_tokens]\n",
    "        sorted_freqs = [sort_by_frequency(f) for f in freqs]\n",
    "        matrix = build_word_document_matrix(docs_tokens)\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n‚úÖ Processing Completed!\\n\")\n",
    "        print(\"üîπ Word Tokens (Doc1):\", docs_tokens[0][:20], \"...\")\n",
    "        print(\"\\nüîπ Sentence Tokens (Doc1):\", sentences[:3], \"...\")\n",
    "        print(\"\\nüîπ Sorted Frequency (Doc1):\", sorted_freqs[0][:10])\n",
    "        print(\"\\nüîπ Word‚ÄìDocument Matrix:\\n\", matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b64f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
